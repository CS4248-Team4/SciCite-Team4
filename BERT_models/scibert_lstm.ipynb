{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO4WiBECOuWg",
        "outputId": "cb608104-525e-42e8-ee92-681c5ab55264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pWpze8_EfBl",
        "outputId": "e4e515fb-8ade-4441-b888-97913f6eb27f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr7Am65moFyj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load SciBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jCKrDJBHLcW"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/archive/train.csv\")\n",
        "val_df = pd.read_csv(\"/content/drive/MyDrive/archive/validation.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/archive/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt-bwhHeVKjr"
      },
      "outputs": [],
      "source": [
        "def vectorise(texts, w2v_model, max_length=40):\n",
        "    vector_size = w2v_model.vector_size\n",
        "    texts_vec = []\n",
        "\n",
        "    words = set(w2v_model.index_to_key)\n",
        "\n",
        "    for text in texts:\n",
        "        sentence_vectors = []\n",
        "\n",
        "        # 遍历句子中的每个词，最多到 max_length\n",
        "        for word in text[:max_length]:\n",
        "          if word in words:\n",
        "              # 如果词在模型的词汇表中，添加它的向量\n",
        "              sentence_vectors.append(w2v_model[word])\n",
        "          else:\n",
        "              # 否则，添加零向量\n",
        "              sentence_vectors.append(np.zeros(vector_size))\n",
        "\n",
        "        # 如果句子长度小于 max_length，用零向量填充剩余的部分\n",
        "        for _ in range(max_length - len(sentence_vectors)):\n",
        "            sentence_vectors.append(np.zeros(vector_size))\n",
        "\n",
        "        # 将句子的词向量列表添加到结果列表中\n",
        "        texts_vec.append(sentence_vectors)\n",
        "\n",
        "    # 将列表转换为三维 NumPy 数组\n",
        "    # 结果形状为 (句子数量, max_length, vector_size)\n",
        "    return np.array(texts_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD0bsvksEsQV"
      },
      "outputs": [],
      "source": [
        "def process_strings(strings):\n",
        "    strings = re.sub(r'\\[[0-9, ]*\\]', '', strings)\n",
        "    strings = re.sub(r'^...', '... ', strings)\n",
        "    strings = word_tokenize(strings.lower())\n",
        "    return strings\n",
        "\n",
        "def process_names(sectionNames):\n",
        "    returned = []\n",
        "    for case in sectionNames:\n",
        "        print(case)\n",
        "        case = case.lower()\n",
        "        case = re.sub(r'^[0-9.]{2,}', '', case)\n",
        "        returned.append(case)\n",
        "    return returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAeBnpkYHRE5"
      },
      "outputs": [],
      "source": [
        "def preprocess_sectionName(sectionName):\n",
        "    sectionName = str(sectionName)\n",
        "    newSectionName = sectionName.lower()\n",
        "\n",
        "    if newSectionName != None:\n",
        "        if \"introduction\" in newSectionName or \"preliminaries\" in newSectionName:\n",
        "            newSectionName = \"introduction\"\n",
        "        elif \"result\" in newSectionName or \"finding\" in newSectionName:\n",
        "            newSectionName = \"results\"\n",
        "        elif \"method\" in newSectionName or \"approach\" in newSectionName:\n",
        "            newSectionName = \"method\"\n",
        "        elif \"discussion\" in newSectionName:\n",
        "            newSectionName = \"discussion\"\n",
        "        elif \"background\" in newSectionName:\n",
        "            newSectionName = \"background\"\n",
        "        elif \"experiment\" in newSectionName or \"setup\" in newSectionName or \"set-up\" in newSectionName or \"set up\" in newSectionName:\n",
        "            newSectionName = \"experiment\"\n",
        "        elif \"related work\" in newSectionName or \"relatedwork\" in newSectionName or \"prior work\" in newSectionName or \"literature review\" in newSectionName:\n",
        "            newSectionName = \"related work\"\n",
        "        elif \"evaluation\" in newSectionName:\n",
        "            newSectionName = \"evaluation\"\n",
        "        elif \"implementation\" in newSectionName:\n",
        "            newSectionName = \"implementation\"\n",
        "        elif \"conclusion\" in newSectionName:\n",
        "            newSectionName = \"conclusion\"\n",
        "        elif \"limitation\" in newSectionName:\n",
        "            newSectionName = \"limitation\"\n",
        "        elif \"appendix\" in newSectionName:\n",
        "            newSectionName = \"appendix\"\n",
        "        elif \"future work\" in newSectionName or \"extension\" in newSectionName:\n",
        "            newSectionName = \"appendix\"\n",
        "        elif \"analysis\" in newSectionName:\n",
        "            newSectionName = \"analysis\"\n",
        "        else:\n",
        "            newSectionName = \"unspecified\"\n",
        "\n",
        "        return newSectionName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kleaKnEVHSZe"
      },
      "outputs": [],
      "source": [
        "train_df[\"sectionName\"] = train_df[\"sectionName\"].apply(preprocess_sectionName)\n",
        "val_df[\"sectionName\"] = val_df[\"sectionName\"].apply(preprocess_sectionName)\n",
        "test_df[\"sectionName\"] = test_df[\"sectionName\"].apply(preprocess_sectionName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3Y1y6suHwOv"
      },
      "outputs": [],
      "source": [
        "sec_name_mapping = {\"discussion\": 0, \"introduction\": 1, \"unspecified\": 2, \"method\": 3,\n",
        "                    \"results\": 4, \"experiment\": 5, \"background\": 6, \"implementation\": 7,\n",
        "                    \"related work\": 8, \"analysis\": 9, \"conclusion\": 10, \"evaluation\": 11,\n",
        "                    \"appendix\": 12, \"limitation\": 13}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu02todVHtYH"
      },
      "outputs": [],
      "source": [
        "train_df_clean = train_df[[\"string\", \"sectionName\", \"label\"]]\n",
        "val_df_clean = val_df[[\"string\", \"sectionName\", \"label\"]]\n",
        "test_df_clean = test_df[[\"string\", \"sectionName\", \"label\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j72_-sPoH1Cl"
      },
      "outputs": [],
      "source": [
        "train_sec = train_df_clean['sectionName'].tolist()\n",
        "train_sec = [sec_name_mapping[label] for label in train_sec]\n",
        "train_text = train_df_clean['string']\n",
        "train_labels = train_df_clean[\"label\"]\n",
        "\n",
        "val_sec = val_df_clean['sectionName'].tolist()\n",
        "val_sec = [sec_name_mapping[label] for label in val_sec]\n",
        "val_text = val_df_clean['string']\n",
        "val_labels = val_df_clean[\"label\"]\n",
        "\n",
        "test_sec = test_df_clean['sectionName'].tolist()\n",
        "test_sec = [sec_name_mapping[label] for label in test_sec]\n",
        "test_text = test_df_clean['string']\n",
        "test_labels = test_df_clean[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rgQ_9DFo-xy"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms4c49z6pCkj"
      },
      "outputs": [],
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FN00R9JMrqU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcPB5kpGpcqU"
      },
      "outputs": [],
      "source": [
        "train_ids = torch.tensor(tokens_train[\"input_ids\"])\n",
        "train_masks = torch.tensor(tokens_train[\"attention_mask\"])\n",
        "\n",
        "val_ids = torch.tensor(tokens_val[\"input_ids\"])\n",
        "val_masks = torch.tensor(tokens_val[\"attention_mask\"])\n",
        "\n",
        "test_ids = torch.tensor(tokens_test[\"input_ids\"])\n",
        "test_masks = torch.tensor(tokens_test[\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hifJh5jcr6rU"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "total_samples = len(train_ids)\n",
        "\n",
        "all_embeddings = []\n",
        "\n",
        "# Process the dataset in batches\n",
        "with torch.no_grad():\n",
        "    for start_idx in range(0, total_samples, batch_size):\n",
        "        print(start_idx)\n",
        "        end_idx = min(start_idx + batch_size, total_samples)\n",
        "        batch_ids = train_ids[start_idx:end_idx]\n",
        "        batch_masks = train_masks[start_idx:end_idx]\n",
        "\n",
        "        # Get embeddings for the current batch\n",
        "        batch_embeddings = model(batch_ids, attention_mask=batch_masks).last_hidden_state\n",
        "\n",
        "        # Append batch embeddings to the list\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "\n",
        "# Concatenate embeddings from all batches along the batch dimension\n",
        "train_embeddings = torch.cat(all_embeddings, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz6tG3BKss5m"
      },
      "outputs": [],
      "source": [
        "total_samples = len(val_ids)\n",
        "\n",
        "all_embeddings = []\n",
        "\n",
        "# Process the dataset in batches\n",
        "with torch.no_grad():\n",
        "    for start_idx in range(0, total_samples, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_samples)\n",
        "        batch_ids = val_ids[start_idx:end_idx]\n",
        "        batch_masks = val_masks[start_idx:end_idx]\n",
        "\n",
        "        # Get embeddings for the current batch\n",
        "        batch_embeddings = model(batch_ids, attention_mask=batch_masks).last_hidden_state\n",
        "\n",
        "        # Append batch embeddings to the list\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "\n",
        "# Concatenate embeddings from all batches along the batch dimension\n",
        "val_embeddings = torch.cat(all_embeddings, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVfi1V4hsbSz"
      },
      "outputs": [],
      "source": [
        "total_samples = len(test_ids)\n",
        "\n",
        "all_embeddings = []\n",
        "\n",
        "# Process the dataset in batches\n",
        "with torch.no_grad():\n",
        "    for start_idx in range(0, total_samples, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_samples)\n",
        "        batch_ids = test_ids[start_idx:end_idx]\n",
        "        batch_masks = test_masks[start_idx:end_idx]\n",
        "\n",
        "        # Get embeddings for the current batch\n",
        "        batch_embeddings = model(batch_ids, attention_mask=batch_masks).last_hidden_state\n",
        "\n",
        "        # Append batch embeddings to the list\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "\n",
        "# Concatenate embeddings from all batches along the batch dimension\n",
        "test_embeddings = torch.cat(all_embeddings, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r-DLmZgZXoY"
      },
      "outputs": [],
      "source": [
        "print(train_df_clean['string'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGtBMB_JZEs5"
      },
      "outputs": [],
      "source": [
        "print(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY6EukFadGP4"
      },
      "outputs": [],
      "source": [
        "# for train set\n",
        "train_text = torch.tensor(train_embeddings, dtype=torch.float32)\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_text = torch.tensor(val_embeddings, dtype=torch.float32)\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_text = torch.tensor(test_embeddings, dtype=torch.float32)\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiJvK4E9c-J3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_text, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_text, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mO2s1llGUNm"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.bilstm(x)\n",
        "\n",
        "        attention_scores = self.attention(out).squeeze(-1)\n",
        "\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "\n",
        "        weighted_sum = torch.sum(out * attention_weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        out = self.fc(weighted_sum)\n",
        "        return out\n",
        "\n",
        "# Parameters\n",
        "input_size = 768  # Input size\n",
        "hidden_size = 128  # Number of neurons in each LSTM layer\n",
        "num_layers = 1  # Number of LSTM layers\n",
        "num_classes = 3  # Number of output classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QE17Nk_fXFF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return running_loss / len(val_loader), correct / total\n",
        "\n",
        "def predict(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "    return predictions\n",
        "\n",
        "model = LSTM(input_size, hidden_size, num_layers, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_dataloader, criterion, optimizer)\n",
        "    val_loss, val_acc = evaluate(model, val_dataloader, criterion)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an4z4gp4njUh"
      },
      "outputs": [],
      "source": [
        "def predict(model, inputs):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return predicted.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxByvVJgh4NS"
      },
      "outputs": [],
      "source": [
        "preds = predict(model, test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRglsNEpODEt"
      },
      "outputs": [],
      "source": [
        "# # model's performance\n",
        "# preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))\n",
        "\n",
        "micro_f1_score = f1_score(test_y, preds, average='micro')\n",
        "print(micro_f1_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}