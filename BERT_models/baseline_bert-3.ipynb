{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNwgPtCCgKx2",
        "outputId": "6b24b218-c272-4f67-ffa9-3569a0c3faa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRqAFLCngHQa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7JsW7mtgHQc"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/archive/train.csv\")\n",
        "val_df = pd.read_csv(\"/content/drive/MyDrive/archive/validation.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/archive/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inZOeDvzgHQd",
        "outputId": "84267204-4770-42dc-d62e-912ce44fd02e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sectionName\n",
              "Discussion                                         1227\n",
              "Introduction                                        833\n",
              "Methods                                             789\n",
              "DISCUSSION                                          481\n",
              "Results                                             359\n",
              "                                                   ... \n",
              "9. Conclusion and future work                         1\n",
              "2.3 Methods                                           1\n",
              "5 Computational Study                                 1\n",
              "Protein samples                                       1\n",
              "2.2 Concepts for improved model version control       1\n",
              "Name: count, Length: 1146, dtype: int64"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate frequency table\n",
        "frequency_table = train_df['sectionName'].value_counts()\n",
        "\n",
        "# Print the frequency table\n",
        "frequency_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5KAc-uKgHQd"
      },
      "outputs": [],
      "source": [
        "def preprocess_sectionName(sectionName):\n",
        "    sectionName = str(sectionName)\n",
        "    newSectionName = sectionName.lower()\n",
        "    # sectionName_split = newSectionName.split()\n",
        "    # pattern = r'\\d+'\n",
        "\n",
        "    # if re.search(pattern, sectionName_split[0]):\n",
        "    #     if len(sectionName_split) > 1:\n",
        "    #         newSectionName = \" \".join(sectionName_split[1:])\n",
        "    #     else:\n",
        "    #         newSectionName = None\n",
        "\n",
        "    if newSectionName != None:\n",
        "        if \"introduction\" in newSectionName or \"preliminaries\" in newSectionName:\n",
        "            newSectionName = \"introduction\"\n",
        "        elif \"result\" in newSectionName or \"finding\" in newSectionName:\n",
        "            newSectionName = \"results\"\n",
        "        elif \"method\" in newSectionName or \"approach\" in newSectionName:\n",
        "            newSectionName = \"method\"\n",
        "        elif \"discussion\" in newSectionName:\n",
        "            newSectionName = \"discussion\"\n",
        "        elif \"background\" in newSectionName:\n",
        "            newSectionName = \"background\"\n",
        "        elif \"experiment\" in newSectionName or \"setup\" in newSectionName or \"set-up\" in newSectionName or \"set up\" in newSectionName:\n",
        "            newSectionName = \"experiment\"\n",
        "        elif \"related work\" in newSectionName or \"relatedwork\" in newSectionName or \"prior work\" in newSectionName or \"literature review\" in newSectionName:\n",
        "            newSectionName = \"related work\"\n",
        "        elif \"evaluation\" in newSectionName:\n",
        "            newSectionName = \"evaluation\"\n",
        "        elif \"implementation\" in newSectionName:\n",
        "            newSectionName = \"implementation\"\n",
        "        elif \"conclusion\" in newSectionName:\n",
        "            newSectionName = \"conclusion\"\n",
        "        elif \"limitation\" in newSectionName:\n",
        "            newSectionName = \"limitation\"\n",
        "        elif \"appendix\" in newSectionName:\n",
        "            newSectionName = \"appendix\"\n",
        "        elif \"future work\" in newSectionName or \"extension\" in newSectionName:\n",
        "            newSectionName = \"appendix\"\n",
        "        elif \"analysis\" in newSectionName:\n",
        "            newSectionName = \"analysis\"\n",
        "        else:\n",
        "            newSectionName = \"unspecified\"\n",
        "\n",
        "        return newSectionName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iQV94oTgHQe",
        "outputId": "ff42cfaf-1b23-4f6c-d1ea-40c63091691f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sectionName\n",
              "discussion        2009\n",
              "introduction      1686\n",
              "unspecified       1580\n",
              "method            1532\n",
              "results            764\n",
              "experiment         291\n",
              "background         124\n",
              "implementation      68\n",
              "related work        61\n",
              "analysis            22\n",
              "conclusion          20\n",
              "evaluation          18\n",
              "appendix            14\n",
              "limitation           5\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"sectionName\"] = train_df[\"sectionName\"].apply(preprocess_sectionName)\n",
        "val_df[\"sectionName\"] = val_df[\"sectionName\"].apply(preprocess_sectionName)\n",
        "test_df[\"sectionName\"] = test_df[\"sectionName\"].apply(preprocess_sectionName)\n",
        "\n",
        "frequency_table = train_df['sectionName'].value_counts()\n",
        "\n",
        "# Print the frequency table\n",
        "frequency_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjn8gBUDgHQe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModel,BertTokenizerFast\n",
        "\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "# bert = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels = 3)\n",
        "\n",
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv3NdXwEgHQf"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModel,BertTokenizerFast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcFkUMIxgHQf"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swQwNwzKgHQf"
      },
      "outputs": [],
      "source": [
        "sec_name_mapping = {\"discussion\": 0, \"introduction\": 1, \"unspecified\": 2, \"method\": 3,\n",
        "                    \"results\": 4, \"experiment\": 5, \"background\": 6, \"implementation\": 7,\n",
        "                    \"related work\": 8, \"analysis\": 9, \"conclusion\": 10, \"evaluation\": 11,\n",
        "                    \"appendix\": 12, \"limitation\": 13}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGHkUblggHQf"
      },
      "outputs": [],
      "source": [
        "train_df_clean = train_df[[\"string\", \"sectionName\", \"label\"]]\n",
        "val_df_clean = val_df[[\"string\", \"sectionName\", \"label\"]]\n",
        "test_df_clean = test_df[[\"string\", \"sectionName\", \"label\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F97_q28gHQg"
      },
      "outputs": [],
      "source": [
        "# train_sec = train_df_clean['sectionName'].tolist()\n",
        "# train_sec = [sec_name_mapping[label] for label in train_sec]\n",
        "train_text = train_df_clean['string']\n",
        "train_labels = train_df_clean[\"label\"]\n",
        "\n",
        "# val_sec = val_df_clean['sectionName'].tolist()\n",
        "# val_sec = [sec_name_mapping[label] for label in val_sec]\n",
        "val_text = val_df_clean['string']\n",
        "val_labels = val_df_clean[\"label\"]\n",
        "\n",
        "# test_sec = test_df_clean['sectionName'].tolist()\n",
        "# test_sec = [sec_name_mapping[label] for label in test_sec]\n",
        "test_text = test_df_clean['string']\n",
        "test_labels = test_df_clean[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1eyDenygHQg"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvREYszQgHQg"
      },
      "outputs": [],
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRNs7r7VgHQg"
      },
      "outputs": [],
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "# train_sec = torch.tensor(train_sec)\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "# val_sec = torch.tensor(val_sec)\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "# test_sec = torch.tensor(test_sec)\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp8m5FwmgHQh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8JPlTHsgHQh"
      },
      "outputs": [],
      "source": [
        "# # freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xSz_pxngHQh"
      },
      "outputs": [],
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "\n",
        "        super(BERT_Arch, self).__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        self.relu =  nn.ReLU()\n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512,3)\n",
        "        self.fc3 = nn.Linear(4,3)\n",
        "\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "        #pass the inputs to the model\n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "\n",
        "        x = self.fc1(cls_hs)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # x = torch.cat((x, sec.float().view(-1, 1)), 1)\n",
        "\n",
        "        # x = self.fc3(x)\n",
        "\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nX0rpCrgHQh"
      },
      "outputs": [],
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb3iq9ezgHQi"
      },
      "outputs": [],
      "source": [
        "# optimizer from hugging face transformers\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pci28oFgHQi",
        "outputId": "fc9503b4-3e9a-4bd9-fcc9-de7071d54c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.20535452 0.56432507 2.51041667]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print(class_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFeFC9ARgHQi"
      },
      "outputs": [],
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights)\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HTrxa5VhEhO"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "    # print(sent_id.shape, mask.shape, sec.shape, labels.shape)\n",
        "\n",
        "    # clear previously calculated gradients\n",
        "    model.zero_grad()\n",
        "    # print(sent_id.shape, mask.shape, sec.shape, labels.shape)\n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "    # print(sent_id.shape, mask.shape, sec.shape, labels.shape)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  print(avg_loss, total_preds)\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2wHIHb8gHQi"
      },
      "outputs": [],
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "\n",
        "  print(\"\\nEvaluating...\")\n",
        "\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "\n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "\n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "\n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLQP-VA_gHQi",
        "outputId": "2063aff8-5e49-438d-9b30-227b02c33754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.9366491700192834 [[-1.1113416  -1.1184449  -1.0668366 ]\n",
            " [-1.1367912  -1.0692683  -1.0909613 ]\n",
            " [-1.0611637  -1.2017632  -1.0404843 ]\n",
            " ...\n",
            " [-1.1856863  -1.1629978  -0.9625551 ]\n",
            " [-0.35344782 -1.6040735  -2.3365388 ]\n",
            " [-0.42011163 -1.2809013  -2.7296872 ]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.937\n",
            "Validation Loss: 0.782\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.7826803889024119 [[-1.4143225  -0.68643373 -1.372227  ]\n",
            " [-0.8082758  -1.024401   -1.632897  ]\n",
            " [-2.5951493  -2.4016607  -0.1805657 ]\n",
            " ...\n",
            " [-1.587745   -1.20713    -0.7000515 ]\n",
            " [-0.27083156 -1.7468596  -2.7656598 ]\n",
            " [-0.8931618  -0.7512741  -2.1296782 ]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.783\n",
            "Validation Loss: 0.685\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.7408165711373207 [[-0.06323797 -3.5367148  -3.4366853 ]\n",
            " [-0.39403975 -1.9748877  -1.6772038 ]\n",
            " [-0.16522603 -2.015512   -3.9609113 ]\n",
            " ...\n",
            " [-1.5827956  -1.0619726  -0.801118  ]\n",
            " [-0.2975192  -1.6241837  -2.8089347 ]\n",
            " [-1.8716235  -0.91757077 -0.80600667]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.741\n",
            "Validation Loss: 0.712\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.7227707245470485 [[-1.9567007  -0.502293   -1.3722551 ]\n",
            " [-3.2869148  -0.14511283 -2.3257885 ]\n",
            " [-0.9539095  -0.7595942  -1.9179232 ]\n",
            " ...\n",
            " [-1.1254313  -0.4488505  -3.2934399 ]\n",
            " [-1.5437088  -0.620258   -1.3918825 ]\n",
            " [-1.0632716  -1.3124181  -0.95319486]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.723\n",
            "Validation Loss: 0.651\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.7224027548086782 [[-1.2878039  -0.7742448  -1.3353269 ]\n",
            " [-0.9619874  -1.6540256  -0.8519344 ]\n",
            " [-0.33136463 -1.5552884  -2.6460865 ]\n",
            " ...\n",
            " [-2.0707831  -0.85853225 -0.7982203 ]\n",
            " [-1.2944912  -2.8281908  -0.4052005 ]\n",
            " [-3.321237   -0.04021432 -5.711319  ]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.722\n",
            "Validation Loss: 0.645\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.6917952002254442 [[-0.7486956  -0.68457496 -3.7848463 ]\n",
            " [-0.6456028  -1.9839076  -1.0843459 ]\n",
            " [-0.29445246 -1.7492338  -2.5114124 ]\n",
            " ...\n",
            " [-2.8976882  -0.08312459 -3.7044811 ]\n",
            " [-2.5278914  -0.09326638 -4.6861596 ]\n",
            " [-1.5804287  -0.5253204  -1.5957946 ]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.692\n",
            "Validation Loss: 0.707\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.6963218975275871 [[-4.3467574  -0.09721584 -2.529597  ]\n",
            " [-0.12766907 -2.2045147  -4.651073  ]\n",
            " [-2.7864964  -0.38185388 -1.363483  ]\n",
            " ...\n",
            " [-0.1580004  -2.416717   -2.865818  ]\n",
            " [-2.87437    -0.42136768 -1.2468818 ]\n",
            " [-4.492554   -0.05287043 -3.2112777 ]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.696\n",
            "Validation Loss: 0.844\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.6854916966378921 [[-6.016858   -0.01241338 -4.615292  ]\n",
            " [-0.9727989  -1.3810154  -0.99248815]\n",
            " [-5.104179   -0.04225487 -3.3437812 ]\n",
            " ...\n",
            " [-0.35949847 -1.2962341  -3.560887  ]\n",
            " [-1.1084459  -1.326024   -0.9053568 ]\n",
            " [-0.86228865 -0.6047541  -3.4547071 ]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.685\n",
            "Validation Loss: 0.722\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.6730193764783066 [[-0.4566549  -1.834537   -1.5754532 ]\n",
            " [-0.15668343 -1.9669365  -5.2705874 ]\n",
            " [-2.8853178  -3.250053   -0.09938858]\n",
            " ...\n",
            " [-3.922406   -0.38606688 -1.2023691 ]\n",
            " [-1.2745266  -1.4595627  -0.71723735]\n",
            " [-2.618149   -0.64034575 -0.91641045]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.673\n",
            "Validation Loss: 0.753\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    257.\n",
            "  Batch   100  of    257.\n",
            "  Batch   150  of    257.\n",
            "  Batch   200  of    257.\n",
            "  Batch   250  of    257.\n",
            "0.6617462474432437 [[-5.1879916  -4.028811   -0.02365639]\n",
            " [-2.6755042  -0.0976496  -3.7230039 ]\n",
            " [-0.17330761 -3.7083774  -2.0054195 ]\n",
            " ...\n",
            " [-0.6936625  -1.2590679  -1.5309085 ]\n",
            " [-0.0932135  -2.447409   -5.9980397 ]\n",
            " [-2.1981359  -0.23074958 -2.353333  ]]\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.662\n",
            "Validation Loss: 0.658\n"
          ]
        }
      ],
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wZyLti4gHQj",
        "outputId": "b450f43f-61e8-42fa-b79e-653acbfdbe52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utasgh9_gHQj"
      },
      "outputs": [],
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b84w8rNgHQj",
        "outputId": "0d497127-a9c3-4fd9-b4a9-23596e362b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       604\n",
            "           1       0.87      0.73      0.79       996\n",
            "           2       0.52      0.78      0.62       259\n",
            "\n",
            "    accuracy                           0.76      1859\n",
            "   macro avg       0.72      0.78      0.74      1859\n",
            "weighted avg       0.79      0.76      0.77      1859\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr7KYdtD7dyP",
        "outputId": "196d621a-0ccf-49c2-9821-453988ee9074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Micro F1 Score: 0.7643894566971491\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "micro_f1 = f1_score(test_y, preds, average='micro')\n",
        "\n",
        "print(\"Micro F1 Score:\", micro_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRV7Q-q-RnA_",
        "outputId": "df2bb456-c3fb-4d14-ccf1-b0c4f1291866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Micro F1 Score: 0.7373121387038225\n"
          ]
        }
      ],
      "source": [
        "macro_f1 = f1_score(test_y, preds, average='macro')\n",
        "\n",
        "print(\"Micro F1 Score:\", macro_f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4 (main, Jul  5 2023, 08:54:11) [Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "6e10138e559af5604874ea8e7a4bef538a58e646e137cbbee6478eecc45f07b0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
